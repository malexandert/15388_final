{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15388 Final\n",
    "#### Jake Parker (jlparker), Marcus Todd (matodd), William Tong (wxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back on this past election, one can clearly see how jobs and the economy played a pivotal role in influencing the voting decisions of many Americans. With the Great Recession of the 2010s, and an increasing shift towards a globalized economy, some industries felt the blunt of those effects through the loss of jobs, and what we would like to see is whether the people most affected by these changes responded with their votes. While obviously it's hard to definitively pin down a cause just based on how people voted, we want to compare labor department data about umeployment in certain sectors and voting data from certain states to see if we can find a trend or come to any interesting conclusions.\n",
    "\n",
    "For our project, we look towards a number of sources to aid in our analysis. First, on the labor statistics front, we looked towards the [Bureau of Labor Statistics](http://www.bls.gov/bls/proghome.htm) and, specifically, the [Bureau of Labor Statistics Public Data API](http://www.bls.gov/developers/api_signature_v2.htm) to collect data on unemployment over various industries and locations. Second, for election data, we turn towards major news sources like [Politico](http://www.politico.com/) and the [New York Times](http://www.nytimes.com) to collect voter numbers from each state during the 2008, 2012, and 2016 elections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Election"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though both Politico and the New York Times, as many news sources do, ultimately draw their data from the Associated Press, API keys for AP aren't free, so we'll have to take a slightly more indirect approach by scraping the news pages themselves for the data instead of getting it directly from AP. Unfortunately, due to formatting changes over the years, it's a little more difficult than we'd like it to be to fetch voter data from the 2008, 2012, and 2016 elections with one script. However, it's not too hard to write scripts for each of those elections individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# List of states and D.C.\n",
    "states = [\n",
    "    'Alabama','Alaska','Arizona','Arkansas','California','Colorado',\n",
    "    'Connecticut','Delaware','District Of Columbia','Florida','Georgia','Hawaii','Idaho', \n",
    "    'Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana',\n",
    "    'Maine','Maryland','Massachusetts','Michigan','Minnesota',\n",
    "    'Mississippi', 'Missouri','Montana','Nebraska','Nevada',\n",
    "    'New Hampshire','New Jersey','New Mexico','New York',\n",
    "    'North Carolina','North Dakota','Ohio',    \n",
    "    'Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "    'South Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "    'Vermont','Virginia','Washington','West Virginia',\n",
    "    'Wisconsin','Wyoming'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll be collecting 2008 data from the New York Times page on the election. In terms of formatting, we're hoping to get mainly the state and the number of votes for the Democratic and Republican party candidates (we'll be ignoring third-parties in this analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Converts data taken from the table in the html\n",
    "    Input: a table row of voting data, in html\n",
    "    Output: a list of data in a more desirable format\n",
    "'''\n",
    "def voter_html_to_data_2008(voter_html):\n",
    "    data = [datum.get_text().strip() for datum in voter_html.find_all('td')]\n",
    "    blue_votes = int(data[2][:-5].replace(',',''))\n",
    "    red_votes = int(data[4][:-5].replace(',',''))\n",
    "    return [blue_votes, red_votes]\n",
    "\n",
    "def get_voter_data_2008():\n",
    "    # Set up our data frame\n",
    "    df = pd.DataFrame(columns=('election_year', 'state', 'dem_votes', 'rep_votes'))\n",
    "    \n",
    "    # Base url we'll be getting data from \n",
    "    base_url = 'http://elections.nytimes.com/2008/results/states/president/'\n",
    "    \n",
    "    # Get state names for url endings\n",
    "    state_urls = sorted([state.lower().replace(' ', '-') for state in states])\n",
    "\n",
    "    # Iterate through the states (except for Alaska and D.C.)\n",
    "    num_states = 0\n",
    "    for state in state_urls:\n",
    "        if state != 'alaska' and state != 'district-of-columbia':\n",
    "            # Get data from the site\n",
    "            response = requests.get(base_url + state + '.html')\n",
    "            election_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Data for all states except for Alaska and D.C.\n",
    "            data_rows = election_soup.find(id='winners-by-county-table').tbody.find_all('tr')\n",
    "            \n",
    "            # Data that every row will have (election year and state)\n",
    "            header_data = ['2008', state]\n",
    "            state_vote_lists = [voter_html_to_data_2008(row) for row in data_rows]\n",
    "            state_vote_counts = [sum([vote[0] for vote in state_vote_lists]), sum([vote[1] for vote in state_vote_lists])]\n",
    "            voter_data = header_data + state_vote_counts\n",
    "            df.loc[num_states] = voter_data\n",
    "            num_states += 1\n",
    "    \n",
    "    # Since Alaska and D.C. don't have counties, we process them slightly differently\n",
    "    # First, Alaska\n",
    "    alaska_html = requests.get('http://elections.nytimes.com/2008/results/states/alaska.html')\n",
    "    alaska_election_soup = BeautifulSoup(alaska_html.text, 'html.parser')\n",
    "    \n",
    "    alaska_obama = alaska_election_soup.find(id='presidential-results-table').tbody.find_all('tr')[:2][1].find_all('td')\n",
    "    alaska_mccain = alaska_election_soup.find(id='presidential-results-table').tbody.find_all('tr')[:2][0].find_all('td')\n",
    "    alaska_blue_votes = int(alaska_obama[1].get_text().strip().replace(',',''))\n",
    "    alaska_red_votes = int(alaska_mccain[2].get_text().strip().replace(',',''))\n",
    "    alaska_data = ['2008', 'alaska', alaska_blue_votes, alaska_red_votes]\n",
    "    df.loc[num_states] = alaska_data\n",
    "    num_states += 1\n",
    "    \n",
    "    # Finally, D.C.\n",
    "    dc_html = requests.get('http://elections.nytimes.com/2008/results/states/district-of-columbia.html')\n",
    "    dc_election_soup = BeautifulSoup(dc_html.text, 'html.parser')\n",
    "\n",
    "    dc_obama = dc_election_soup.find(id='presidential-results-table').tbody.find_all('tr')[:2][0].find_all('td')\n",
    "    dc_mccain = dc_election_soup.find(id='presidential-results-table').tbody.find_all('tr')[:2][1].find_all('td')\n",
    "    dc_blue_votes = int(dc_obama[2].get_text().strip().replace(',',''))\n",
    "    dc_red_votes = int(dc_mccain[1].get_text().strip().replace(',',''))\n",
    "    dc_data = ['2008', 'district-of-columbia', dc_blue_votes, dc_red_votes]\n",
    "    df.loc[num_states] = dc_data\n",
    "    num_states += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2008 = get_voter_data_2008()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>election_year</th>\n",
       "      <th>state</th>\n",
       "      <th>dem_votes</th>\n",
       "      <th>rep_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>alabama</td>\n",
       "      <td>811764.0</td>\n",
       "      <td>1264879.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>arizona</td>\n",
       "      <td>948648.0</td>\n",
       "      <td>1132560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>418049.0</td>\n",
       "      <td>632672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>california</td>\n",
       "      <td>7441458.0</td>\n",
       "      <td>4554643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>colorado</td>\n",
       "      <td>1216793.0</td>\n",
       "      <td>1020135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008</td>\n",
       "      <td>connecticut</td>\n",
       "      <td>994320.0</td>\n",
       "      <td>627688.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008</td>\n",
       "      <td>delaware</td>\n",
       "      <td>255394.0</td>\n",
       "      <td>152356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008</td>\n",
       "      <td>florida</td>\n",
       "      <td>4143957.0</td>\n",
       "      <td>3939380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>georgia</td>\n",
       "      <td>1843452.0</td>\n",
       "      <td>2048244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008</td>\n",
       "      <td>hawaii</td>\n",
       "      <td>324918.0</td>\n",
       "      <td>120309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2008</td>\n",
       "      <td>idaho</td>\n",
       "      <td>235219.0</td>\n",
       "      <td>400989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2008</td>\n",
       "      <td>illinois</td>\n",
       "      <td>3319237.0</td>\n",
       "      <td>1981158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2008</td>\n",
       "      <td>indiana</td>\n",
       "      <td>1367264.0</td>\n",
       "      <td>1341101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2008</td>\n",
       "      <td>iowa</td>\n",
       "      <td>818240.0</td>\n",
       "      <td>677508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008</td>\n",
       "      <td>kansas</td>\n",
       "      <td>499979.0</td>\n",
       "      <td>685541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2008</td>\n",
       "      <td>kentucky</td>\n",
       "      <td>746510.0</td>\n",
       "      <td>1043264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2008</td>\n",
       "      <td>louisiana</td>\n",
       "      <td>780981.0</td>\n",
       "      <td>1147603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2008</td>\n",
       "      <td>maine</td>\n",
       "      <td>421481.0</td>\n",
       "      <td>296192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2008</td>\n",
       "      <td>maryland</td>\n",
       "      <td>1612692.0</td>\n",
       "      <td>956663.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2008</td>\n",
       "      <td>massachusetts</td>\n",
       "      <td>1891083.0</td>\n",
       "      <td>1104284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2008</td>\n",
       "      <td>michigan</td>\n",
       "      <td>2867680.0</td>\n",
       "      <td>2044405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2008</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>1573323.0</td>\n",
       "      <td>1275400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2008</td>\n",
       "      <td>mississippi</td>\n",
       "      <td>520864.0</td>\n",
       "      <td>687266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2008</td>\n",
       "      <td>missouri</td>\n",
       "      <td>1442180.0</td>\n",
       "      <td>1445812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2008</td>\n",
       "      <td>montana</td>\n",
       "      <td>229725.0</td>\n",
       "      <td>241816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2008</td>\n",
       "      <td>nebraska</td>\n",
       "      <td>329132.0</td>\n",
       "      <td>448801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2008</td>\n",
       "      <td>nevada</td>\n",
       "      <td>531884.0</td>\n",
       "      <td>411988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2008</td>\n",
       "      <td>new-hampshire</td>\n",
       "      <td>384591.0</td>\n",
       "      <td>316937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2008</td>\n",
       "      <td>new-jersey</td>\n",
       "      <td>2085051.0</td>\n",
       "      <td>1545495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2008</td>\n",
       "      <td>new-mexico</td>\n",
       "      <td>458754.0</td>\n",
       "      <td>340857.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2008</td>\n",
       "      <td>new-york</td>\n",
       "      <td>4363386.0</td>\n",
       "      <td>2576360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2008</td>\n",
       "      <td>north-carolina</td>\n",
       "      <td>2123390.0</td>\n",
       "      <td>2109698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2008</td>\n",
       "      <td>north-dakota</td>\n",
       "      <td>141113.0</td>\n",
       "      <td>168523.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2008</td>\n",
       "      <td>ohio</td>\n",
       "      <td>2708685.0</td>\n",
       "      <td>2501855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2008</td>\n",
       "      <td>oklahoma</td>\n",
       "      <td>502294.0</td>\n",
       "      <td>959745.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2008</td>\n",
       "      <td>oregon</td>\n",
       "      <td>978605.0</td>\n",
       "      <td>699673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2008</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>3192316.0</td>\n",
       "      <td>2586496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2008</td>\n",
       "      <td>rhode-island</td>\n",
       "      <td>275488.0</td>\n",
       "      <td>152502.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2008</td>\n",
       "      <td>south-carolina</td>\n",
       "      <td>850121.0</td>\n",
       "      <td>1018756.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2008</td>\n",
       "      <td>south-dakota</td>\n",
       "      <td>170886.0</td>\n",
       "      <td>203019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2008</td>\n",
       "      <td>tennessee</td>\n",
       "      <td>1081074.0</td>\n",
       "      <td>1470160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2008</td>\n",
       "      <td>texas</td>\n",
       "      <td>3521164.0</td>\n",
       "      <td>4467748.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2008</td>\n",
       "      <td>utah</td>\n",
       "      <td>317063.0</td>\n",
       "      <td>568610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2008</td>\n",
       "      <td>vermont</td>\n",
       "      <td>219105.0</td>\n",
       "      <td>98791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2008</td>\n",
       "      <td>virginia</td>\n",
       "      <td>1958370.0</td>\n",
       "      <td>1726053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2008</td>\n",
       "      <td>washington</td>\n",
       "      <td>1547632.0</td>\n",
       "      <td>1097176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2008</td>\n",
       "      <td>west-virginia</td>\n",
       "      <td>301438.0</td>\n",
       "      <td>394278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2008</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>1670474.0</td>\n",
       "      <td>1258181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008</td>\n",
       "      <td>wyoming</td>\n",
       "      <td>80496.0</td>\n",
       "      <td>160639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2008</td>\n",
       "      <td>alaska</td>\n",
       "      <td>122485.0</td>\n",
       "      <td>192631.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2008</td>\n",
       "      <td>district-of-columbia</td>\n",
       "      <td>210403.0</td>\n",
       "      <td>14821.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   election_year                 state  dem_votes  rep_votes\n",
       "0           2008               alabama   811764.0  1264879.0\n",
       "1           2008               arizona   948648.0  1132560.0\n",
       "2           2008              arkansas   418049.0   632672.0\n",
       "3           2008            california  7441458.0  4554643.0\n",
       "4           2008              colorado  1216793.0  1020135.0\n",
       "5           2008           connecticut   994320.0   627688.0\n",
       "6           2008              delaware   255394.0   152356.0\n",
       "7           2008               florida  4143957.0  3939380.0\n",
       "8           2008               georgia  1843452.0  2048244.0\n",
       "9           2008                hawaii   324918.0   120309.0\n",
       "10          2008                 idaho   235219.0   400989.0\n",
       "11          2008              illinois  3319237.0  1981158.0\n",
       "12          2008               indiana  1367264.0  1341101.0\n",
       "13          2008                  iowa   818240.0   677508.0\n",
       "14          2008                kansas   499979.0   685541.0\n",
       "15          2008              kentucky   746510.0  1043264.0\n",
       "16          2008             louisiana   780981.0  1147603.0\n",
       "17          2008                 maine   421481.0   296192.0\n",
       "18          2008              maryland  1612692.0   956663.0\n",
       "19          2008         massachusetts  1891083.0  1104284.0\n",
       "20          2008              michigan  2867680.0  2044405.0\n",
       "21          2008             minnesota  1573323.0  1275400.0\n",
       "22          2008           mississippi   520864.0   687266.0\n",
       "23          2008              missouri  1442180.0  1445812.0\n",
       "24          2008               montana   229725.0   241816.0\n",
       "25          2008              nebraska   329132.0   448801.0\n",
       "26          2008                nevada   531884.0   411988.0\n",
       "27          2008         new-hampshire   384591.0   316937.0\n",
       "28          2008            new-jersey  2085051.0  1545495.0\n",
       "29          2008            new-mexico   458754.0   340857.0\n",
       "30          2008              new-york  4363386.0  2576360.0\n",
       "31          2008        north-carolina  2123390.0  2109698.0\n",
       "32          2008          north-dakota   141113.0   168523.0\n",
       "33          2008                  ohio  2708685.0  2501855.0\n",
       "34          2008              oklahoma   502294.0   959745.0\n",
       "35          2008                oregon   978605.0   699673.0\n",
       "36          2008          pennsylvania  3192316.0  2586496.0\n",
       "37          2008          rhode-island   275488.0   152502.0\n",
       "38          2008        south-carolina   850121.0  1018756.0\n",
       "39          2008          south-dakota   170886.0   203019.0\n",
       "40          2008             tennessee  1081074.0  1470160.0\n",
       "41          2008                 texas  3521164.0  4467748.0\n",
       "42          2008                  utah   317063.0   568610.0\n",
       "43          2008               vermont   219105.0    98791.0\n",
       "44          2008              virginia  1958370.0  1726053.0\n",
       "45          2008            washington  1547632.0  1097176.0\n",
       "46          2008         west-virginia   301438.0   394278.0\n",
       "47          2008             wisconsin  1670474.0  1258181.0\n",
       "48          2008               wyoming    80496.0   160639.0\n",
       "49          2008                alaska   122485.0   192631.0\n",
       "50          2008  district-of-columbia   210403.0    14821.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2012, we'll switch to Politico, since the website formatting is slightly easier to parse than the interactive graphics on the New York Times site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_voter_data_2012():\n",
    "    # Set up our data frame\n",
    "    df = pd.DataFrame(columns=('election_year', 'state', 'dem_votes', 'rep_votes'))\n",
    "    \n",
    "    # Base url we'll be getting data from\n",
    "    base_url = 'http://www.politico.com/2012-election/results/president/'\n",
    "    \n",
    "    # Get state names for url endings\n",
    "    state_urls = sorted([state.lower().replace(' ', '-') for state in states])\n",
    "    \n",
    "    # Once again, Alaska and D.C. are slightly different\n",
    "    num_states = 0\n",
    "    for state in state_urls:\n",
    "        # Get data from site\n",
    "        try:\n",
    "            response = requests.get(base_url + state + '/')\n",
    "        except ConnectionError:\n",
    "            time.sleep(2)\n",
    "            response = requests.get(base_url + state + '/')\n",
    "        election_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        header_data = ['2012', state]\n",
    "        data = election_soup.find('div', class_='state-results-macro').table.tbody\n",
    "        blue_votes = int(data.find(class_='party-democrat').find(class_='results-popular').get_text().strip().replace(',',''))\n",
    "        red_votes = int(data.find(class_='party-republican').find(class_='results-popular').get_text().strip().replace(',',''))\n",
    "        voter_data = header_data + [blue_votes, red_votes]\n",
    "        df.loc[num_states] = voter_data\n",
    "        num_states += 1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2012 = get_voter_data_2012()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>election_year</th>\n",
       "      <th>state</th>\n",
       "      <th>dem_votes</th>\n",
       "      <th>rep_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>alabama</td>\n",
       "      <td>793620.0</td>\n",
       "      <td>1252453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>alaska</td>\n",
       "      <td>102138.0</td>\n",
       "      <td>136848.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>arizona</td>\n",
       "      <td>930669.0</td>\n",
       "      <td>1143051.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>389699.0</td>\n",
       "      <td>638467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>california</td>\n",
       "      <td>6493924.0</td>\n",
       "      <td>4202127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012</td>\n",
       "      <td>colorado</td>\n",
       "      <td>1238490.0</td>\n",
       "      <td>1125391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012</td>\n",
       "      <td>connecticut</td>\n",
       "      <td>912531.0</td>\n",
       "      <td>631432.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012</td>\n",
       "      <td>delaware</td>\n",
       "      <td>242547.0</td>\n",
       "      <td>165476.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012</td>\n",
       "      <td>district-of-columbia</td>\n",
       "      <td>222332.0</td>\n",
       "      <td>17337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012</td>\n",
       "      <td>florida</td>\n",
       "      <td>4235270.0</td>\n",
       "      <td>4162081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012</td>\n",
       "      <td>georgia</td>\n",
       "      <td>1761761.0</td>\n",
       "      <td>2070221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012</td>\n",
       "      <td>hawaii</td>\n",
       "      <td>303090.0</td>\n",
       "      <td>119494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>idaho</td>\n",
       "      <td>212560.0</td>\n",
       "      <td>420750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2012</td>\n",
       "      <td>illinois</td>\n",
       "      <td>2916811.0</td>\n",
       "      <td>2090116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012</td>\n",
       "      <td>indiana</td>\n",
       "      <td>1140425.0</td>\n",
       "      <td>1412620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012</td>\n",
       "      <td>iowa</td>\n",
       "      <td>816429.0</td>\n",
       "      <td>727928.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012</td>\n",
       "      <td>kansas</td>\n",
       "      <td>427918.0</td>\n",
       "      <td>678719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012</td>\n",
       "      <td>kentucky</td>\n",
       "      <td>679340.0</td>\n",
       "      <td>1087127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012</td>\n",
       "      <td>louisiana</td>\n",
       "      <td>808496.0</td>\n",
       "      <td>1152460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012</td>\n",
       "      <td>maine</td>\n",
       "      <td>397754.0</td>\n",
       "      <td>290437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2012</td>\n",
       "      <td>maryland</td>\n",
       "      <td>1527686.0</td>\n",
       "      <td>904970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012</td>\n",
       "      <td>massachusetts</td>\n",
       "      <td>1900575.0</td>\n",
       "      <td>1177370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2012</td>\n",
       "      <td>michigan</td>\n",
       "      <td>2561911.0</td>\n",
       "      <td>2112673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>1547668.0</td>\n",
       "      <td>1321575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2012</td>\n",
       "      <td>mississippi</td>\n",
       "      <td>528260.0</td>\n",
       "      <td>674302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2012</td>\n",
       "      <td>missouri</td>\n",
       "      <td>1215031.0</td>\n",
       "      <td>1478961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012</td>\n",
       "      <td>montana</td>\n",
       "      <td>200489.0</td>\n",
       "      <td>264974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2012</td>\n",
       "      <td>nebraska</td>\n",
       "      <td>289154.0</td>\n",
       "      <td>462972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2012</td>\n",
       "      <td>nevada</td>\n",
       "      <td>528801.0</td>\n",
       "      <td>462422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2012</td>\n",
       "      <td>new-hampshire</td>\n",
       "      <td>368529.0</td>\n",
       "      <td>327870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2012</td>\n",
       "      <td>new-jersey</td>\n",
       "      <td>1960744.0</td>\n",
       "      <td>1383233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2012</td>\n",
       "      <td>new-mexico</td>\n",
       "      <td>408312.0</td>\n",
       "      <td>331915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2012</td>\n",
       "      <td>new-york</td>\n",
       "      <td>3875826.0</td>\n",
       "      <td>2226637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2012</td>\n",
       "      <td>north-carolina</td>\n",
       "      <td>2178388.0</td>\n",
       "      <td>2275853.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2012</td>\n",
       "      <td>north-dakota</td>\n",
       "      <td>124490.0</td>\n",
       "      <td>187586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2012</td>\n",
       "      <td>ohio</td>\n",
       "      <td>2697260.0</td>\n",
       "      <td>2593779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2012</td>\n",
       "      <td>oklahoma</td>\n",
       "      <td>442647.0</td>\n",
       "      <td>889372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2012</td>\n",
       "      <td>oregon</td>\n",
       "      <td>937321.0</td>\n",
       "      <td>733743.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2012</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>2907448.0</td>\n",
       "      <td>2619583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2012</td>\n",
       "      <td>rhode-island</td>\n",
       "      <td>274342.0</td>\n",
       "      <td>155355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2012</td>\n",
       "      <td>south-carolina</td>\n",
       "      <td>845756.0</td>\n",
       "      <td>1049507.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2012</td>\n",
       "      <td>south-dakota</td>\n",
       "      <td>144988.0</td>\n",
       "      <td>210541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2012</td>\n",
       "      <td>tennessee</td>\n",
       "      <td>953043.0</td>\n",
       "      <td>1453097.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2012</td>\n",
       "      <td>texas</td>\n",
       "      <td>3294440.0</td>\n",
       "      <td>4555799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2012</td>\n",
       "      <td>utah</td>\n",
       "      <td>229463.0</td>\n",
       "      <td>671747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2012</td>\n",
       "      <td>vermont</td>\n",
       "      <td>199259.0</td>\n",
       "      <td>92700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2012</td>\n",
       "      <td>virginia</td>\n",
       "      <td>1905528.0</td>\n",
       "      <td>1789618.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2012</td>\n",
       "      <td>washington</td>\n",
       "      <td>1620432.0</td>\n",
       "      <td>1210369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2012</td>\n",
       "      <td>west-virginia</td>\n",
       "      <td>234925.0</td>\n",
       "      <td>412406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2012</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>1613950.0</td>\n",
       "      <td>1408746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2012</td>\n",
       "      <td>wyoming</td>\n",
       "      <td>68780.0</td>\n",
       "      <td>170265.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   election_year                 state  dem_votes  rep_votes\n",
       "0           2012               alabama   793620.0  1252453.0\n",
       "1           2012                alaska   102138.0   136848.0\n",
       "2           2012               arizona   930669.0  1143051.0\n",
       "3           2012              arkansas   389699.0   638467.0\n",
       "4           2012            california  6493924.0  4202127.0\n",
       "5           2012              colorado  1238490.0  1125391.0\n",
       "6           2012           connecticut   912531.0   631432.0\n",
       "7           2012              delaware   242547.0   165476.0\n",
       "8           2012  district-of-columbia   222332.0    17337.0\n",
       "9           2012               florida  4235270.0  4162081.0\n",
       "10          2012               georgia  1761761.0  2070221.0\n",
       "11          2012                hawaii   303090.0   119494.0\n",
       "12          2012                 idaho   212560.0   420750.0\n",
       "13          2012              illinois  2916811.0  2090116.0\n",
       "14          2012               indiana  1140425.0  1412620.0\n",
       "15          2012                  iowa   816429.0   727928.0\n",
       "16          2012                kansas   427918.0   678719.0\n",
       "17          2012              kentucky   679340.0  1087127.0\n",
       "18          2012             louisiana   808496.0  1152460.0\n",
       "19          2012                 maine   397754.0   290437.0\n",
       "20          2012              maryland  1527686.0   904970.0\n",
       "21          2012         massachusetts  1900575.0  1177370.0\n",
       "22          2012              michigan  2561911.0  2112673.0\n",
       "23          2012             minnesota  1547668.0  1321575.0\n",
       "24          2012           mississippi   528260.0   674302.0\n",
       "25          2012              missouri  1215031.0  1478961.0\n",
       "26          2012               montana   200489.0   264974.0\n",
       "27          2012              nebraska   289154.0   462972.0\n",
       "28          2012                nevada   528801.0   462422.0\n",
       "29          2012         new-hampshire   368529.0   327870.0\n",
       "30          2012            new-jersey  1960744.0  1383233.0\n",
       "31          2012            new-mexico   408312.0   331915.0\n",
       "32          2012              new-york  3875826.0  2226637.0\n",
       "33          2012        north-carolina  2178388.0  2275853.0\n",
       "34          2012          north-dakota   124490.0   187586.0\n",
       "35          2012                  ohio  2697260.0  2593779.0\n",
       "36          2012              oklahoma   442647.0   889372.0\n",
       "37          2012                oregon   937321.0   733743.0\n",
       "38          2012          pennsylvania  2907448.0  2619583.0\n",
       "39          2012          rhode-island   274342.0   155355.0\n",
       "40          2012        south-carolina   845756.0  1049507.0\n",
       "41          2012          south-dakota   144988.0   210541.0\n",
       "42          2012             tennessee   953043.0  1453097.0\n",
       "43          2012                 texas  3294440.0  4555799.0\n",
       "44          2012                  utah   229463.0   671747.0\n",
       "45          2012               vermont   199259.0    92700.0\n",
       "46          2012              virginia  1905528.0  1789618.0\n",
       "47          2012            washington  1620432.0  1210369.0\n",
       "48          2012         west-virginia   234925.0   412406.0\n",
       "49          2012             wisconsin  1613950.0  1408746.0\n",
       "50          2012               wyoming    68780.0   170265.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for 2016, we'll be using Politico again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def voter_html_to_data_2016(voter_html):\n",
    "    blue_votes = int(voter_html.find(class_='type-democrat').find(class_='results-popular').get_text().replace(',',''))\n",
    "    red_votes = int(voter_html.find(class_='type-republican').find(class_='results-popular').get_text().replace(',',''))\n",
    "    return [blue_votes, red_votes]\n",
    "\n",
    "def get_voter_data_2016():\n",
    "    # Set up our data frame\n",
    "    df = pd.DataFrame(columns=('election_year', 'state', 'dem_votes', 'rep_votes'))\n",
    "    \n",
    "    # Base url we'll be getting data from\n",
    "    base_url = 'http://www.politico.com/2016-election/results/map/president/'\n",
    "    \n",
    "    # Get state names for url endings\n",
    "    state_urls = sorted([state.lower().replace(' ', '-') for state in states])\n",
    "    \n",
    "    # This time, Alaska and D.C. aren't different!\n",
    "    num_states = 0\n",
    "    for state in state_urls:\n",
    "        # Get data from site\n",
    "        response = requests.get(base_url + state + '/')\n",
    "        election_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        data = election_soup.select('section.content-group.election-intro')[0].find('div', class_='overall')\n",
    "        header_data = ['2016', state]\n",
    "        voter_data = header_data + voter_html_to_data_2016(data)\n",
    "        df.loc[num_states] = voter_data\n",
    "        num_states += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2016 = get_voter_data_2016()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>election_year</th>\n",
       "      <th>state</th>\n",
       "      <th>dem_votes</th>\n",
       "      <th>rep_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>alabama</td>\n",
       "      <td>718084.0</td>\n",
       "      <td>1306925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>alaska</td>\n",
       "      <td>93007.0</td>\n",
       "      <td>130415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>arizona</td>\n",
       "      <td>936250.0</td>\n",
       "      <td>1021154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>arkansas</td>\n",
       "      <td>378729.0</td>\n",
       "      <td>677904.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>california</td>\n",
       "      <td>7362490.0</td>\n",
       "      <td>3916209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>colorado</td>\n",
       "      <td>1208095.0</td>\n",
       "      <td>1136354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>connecticut</td>\n",
       "      <td>884432.0</td>\n",
       "      <td>668266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>delaware</td>\n",
       "      <td>235581.0</td>\n",
       "      <td>185103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>district-of-columbia</td>\n",
       "      <td>260223.0</td>\n",
       "      <td>11553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>florida</td>\n",
       "      <td>4485745.0</td>\n",
       "      <td>4605515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016</td>\n",
       "      <td>georgia</td>\n",
       "      <td>1837300.0</td>\n",
       "      <td>2068623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016</td>\n",
       "      <td>hawaii</td>\n",
       "      <td>251853.0</td>\n",
       "      <td>121648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016</td>\n",
       "      <td>idaho</td>\n",
       "      <td>189677.0</td>\n",
       "      <td>407199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016</td>\n",
       "      <td>illinois</td>\n",
       "      <td>2977498.0</td>\n",
       "      <td>2118179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016</td>\n",
       "      <td>indiana</td>\n",
       "      <td>1031953.0</td>\n",
       "      <td>1556220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016</td>\n",
       "      <td>iowa</td>\n",
       "      <td>650790.0</td>\n",
       "      <td>798923.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>kansas</td>\n",
       "      <td>414788.0</td>\n",
       "      <td>656009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016</td>\n",
       "      <td>kentucky</td>\n",
       "      <td>628834.0</td>\n",
       "      <td>1202942.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016</td>\n",
       "      <td>louisiana</td>\n",
       "      <td>779535.0</td>\n",
       "      <td>1178004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016</td>\n",
       "      <td>maine</td>\n",
       "      <td>354873.0</td>\n",
       "      <td>334838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016</td>\n",
       "      <td>maryland</td>\n",
       "      <td>1497951.0</td>\n",
       "      <td>873646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016</td>\n",
       "      <td>massachusetts</td>\n",
       "      <td>1964768.0</td>\n",
       "      <td>1083069.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016</td>\n",
       "      <td>michigan</td>\n",
       "      <td>2268193.0</td>\n",
       "      <td>2279805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>1366676.0</td>\n",
       "      <td>1322891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016</td>\n",
       "      <td>mississippi</td>\n",
       "      <td>462001.0</td>\n",
       "      <td>678457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016</td>\n",
       "      <td>missouri</td>\n",
       "      <td>1054889.0</td>\n",
       "      <td>1585753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016</td>\n",
       "      <td>montana</td>\n",
       "      <td>174521.0</td>\n",
       "      <td>274120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016</td>\n",
       "      <td>nebraska</td>\n",
       "      <td>273858.0</td>\n",
       "      <td>485819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016</td>\n",
       "      <td>nevada</td>\n",
       "      <td>537753.0</td>\n",
       "      <td>511319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016</td>\n",
       "      <td>new-hampshire</td>\n",
       "      <td>348521.0</td>\n",
       "      <td>345789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2016</td>\n",
       "      <td>new-jersey</td>\n",
       "      <td>2021756.0</td>\n",
       "      <td>1535513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2016</td>\n",
       "      <td>new-mexico</td>\n",
       "      <td>380724.0</td>\n",
       "      <td>315875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2016</td>\n",
       "      <td>new-york</td>\n",
       "      <td>4143874.0</td>\n",
       "      <td>2640570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2016</td>\n",
       "      <td>north-carolina</td>\n",
       "      <td>2162074.0</td>\n",
       "      <td>2339603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2016</td>\n",
       "      <td>north-dakota</td>\n",
       "      <td>93526.0</td>\n",
       "      <td>216133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2016</td>\n",
       "      <td>ohio</td>\n",
       "      <td>2317001.0</td>\n",
       "      <td>2771984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2016</td>\n",
       "      <td>oklahoma</td>\n",
       "      <td>419788.0</td>\n",
       "      <td>947934.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2016</td>\n",
       "      <td>oregon</td>\n",
       "      <td>934631.0</td>\n",
       "      <td>742506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2016</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>2844705.0</td>\n",
       "      <td>2912941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2016</td>\n",
       "      <td>rhode-island</td>\n",
       "      <td>249902.0</td>\n",
       "      <td>179421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2016</td>\n",
       "      <td>south-carolina</td>\n",
       "      <td>849469.0</td>\n",
       "      <td>1143611.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2016</td>\n",
       "      <td>south-dakota</td>\n",
       "      <td>117442.0</td>\n",
       "      <td>227701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2016</td>\n",
       "      <td>tennessee</td>\n",
       "      <td>867110.0</td>\n",
       "      <td>1517402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2016</td>\n",
       "      <td>texas</td>\n",
       "      <td>3867816.0</td>\n",
       "      <td>4681590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2016</td>\n",
       "      <td>utah</td>\n",
       "      <td>274188.0</td>\n",
       "      <td>452086.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2016</td>\n",
       "      <td>vermont</td>\n",
       "      <td>178179.0</td>\n",
       "      <td>95053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2016</td>\n",
       "      <td>virginia</td>\n",
       "      <td>1916845.0</td>\n",
       "      <td>1731156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2016</td>\n",
       "      <td>washington</td>\n",
       "      <td>1610524.0</td>\n",
       "      <td>1129120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2016</td>\n",
       "      <td>west-virginia</td>\n",
       "      <td>187457.0</td>\n",
       "      <td>486198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2016</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>1382210.0</td>\n",
       "      <td>1409467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2016</td>\n",
       "      <td>wyoming</td>\n",
       "      <td>55949.0</td>\n",
       "      <td>174248.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   election_year                 state  dem_votes  rep_votes\n",
       "0           2016               alabama   718084.0  1306925.0\n",
       "1           2016                alaska    93007.0   130415.0\n",
       "2           2016               arizona   936250.0  1021154.0\n",
       "3           2016              arkansas   378729.0   677904.0\n",
       "4           2016            california  7362490.0  3916209.0\n",
       "5           2016              colorado  1208095.0  1136354.0\n",
       "6           2016           connecticut   884432.0   668266.0\n",
       "7           2016              delaware   235581.0   185103.0\n",
       "8           2016  district-of-columbia   260223.0    11553.0\n",
       "9           2016               florida  4485745.0  4605515.0\n",
       "10          2016               georgia  1837300.0  2068623.0\n",
       "11          2016                hawaii   251853.0   121648.0\n",
       "12          2016                 idaho   189677.0   407199.0\n",
       "13          2016              illinois  2977498.0  2118179.0\n",
       "14          2016               indiana  1031953.0  1556220.0\n",
       "15          2016                  iowa   650790.0   798923.0\n",
       "16          2016                kansas   414788.0   656009.0\n",
       "17          2016              kentucky   628834.0  1202942.0\n",
       "18          2016             louisiana   779535.0  1178004.0\n",
       "19          2016                 maine   354873.0   334838.0\n",
       "20          2016              maryland  1497951.0   873646.0\n",
       "21          2016         massachusetts  1964768.0  1083069.0\n",
       "22          2016              michigan  2268193.0  2279805.0\n",
       "23          2016             minnesota  1366676.0  1322891.0\n",
       "24          2016           mississippi   462001.0   678457.0\n",
       "25          2016              missouri  1054889.0  1585753.0\n",
       "26          2016               montana   174521.0   274120.0\n",
       "27          2016              nebraska   273858.0   485819.0\n",
       "28          2016                nevada   537753.0   511319.0\n",
       "29          2016         new-hampshire   348521.0   345789.0\n",
       "30          2016            new-jersey  2021756.0  1535513.0\n",
       "31          2016            new-mexico   380724.0   315875.0\n",
       "32          2016              new-york  4143874.0  2640570.0\n",
       "33          2016        north-carolina  2162074.0  2339603.0\n",
       "34          2016          north-dakota    93526.0   216133.0\n",
       "35          2016                  ohio  2317001.0  2771984.0\n",
       "36          2016              oklahoma   419788.0   947934.0\n",
       "37          2016                oregon   934631.0   742506.0\n",
       "38          2016          pennsylvania  2844705.0  2912941.0\n",
       "39          2016          rhode-island   249902.0   179421.0\n",
       "40          2016        south-carolina   849469.0  1143611.0\n",
       "41          2016          south-dakota   117442.0   227701.0\n",
       "42          2016             tennessee   867110.0  1517402.0\n",
       "43          2016                 texas  3867816.0  4681590.0\n",
       "44          2016                  utah   274188.0   452086.0\n",
       "45          2016               vermont   178179.0    95053.0\n",
       "46          2016              virginia  1916845.0  1731156.0\n",
       "47          2016            washington  1610524.0  1129120.0\n",
       "48          2016         west-virginia   187457.0   486198.0\n",
       "49          2016             wisconsin  1382210.0  1409467.0\n",
       "50          2016               wyoming    55949.0   174248.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in our analysis, we'll create a simple function to find the percent of the state that voted blue, given an election year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_percent_blue(state, year):\n",
    "    if year == 2008:\n",
    "        df = df_2008\n",
    "    elif year == 2012:\n",
    "        df = df_2012\n",
    "    else:\n",
    "        df = df_2016\n",
    "    df = df.set_index('state')\n",
    "    dem_votes = df.at[state,'dem_votes']\n",
    "    rep_votes = df.at[state, 'rep_votes']\n",
    "    return dem_votes / (dem_votes + rep_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60713285679\n",
      "0.652778303597\n"
     ]
    }
   ],
   "source": [
    "print find_percent_blue('california', 2012)\n",
    "print find_percent_blue('california', 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scraping Labor Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, string\n",
    "import requests, tqdm\n",
    "from string import join, split\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the Bureau of Labor Statistics Public API is unintuitive, to say the least. Essentially, each unique dataset that the BLS keeps has a serial ID that correesponds to it. Querying the data from the API involves figuring out exactly which dataset you want, and then translating that to a serial ID. The code below allows us to input requests in a reasonable format like JSON and have that get converted to the serial ID that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opand(x,y): \n",
    "    return x and y\n",
    "\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "\n",
    "def nest(keys, d_keys, d):\n",
    "    assert (len(keys) == len(d_keys))\n",
    "    iter_format = lambda (i, d_key) : \"for x%d in d['%s']\" % (i, d_key)\n",
    "    iters = join(map(iter_format, enumerate(d_keys)), sep=\" \")\n",
    "    arg_format = lambda (i) : '(\"' + keys[i] + '\", x' + str(i) + ')'\n",
    "    args = join(map(arg_format, range(len(keys))), sep=\", \")\n",
    "    return eval('[dict([' +  args +  ']) ' + iters + ']')\n",
    "\n",
    "\n",
    "# 500 requests per day\n",
    "# 50 series per request\n",
    "# 20 years per request\n",
    "def request(series):\n",
    "    \"\"\"\n",
    "    brief:\n",
    "        - requests series in batches of 15, returns list of json_data, \n",
    "          one for each request\n",
    "    args:\n",
    "        - series : list of bls series id's\n",
    "    \"\"\"\n",
    "    bls_url = 'http://api.bls.gov/publicAPI/v2/timeseries/data/'\n",
    "    regKeys = [\"bc2b9775e9794f37a23c0f6b2a4659b1\", \n",
    "               \"8874280f2cc142f3833754ad3cb6855c\",\n",
    "               \"dacaff7ea67b466685622348e9807f8f\"]\n",
    "    json_data = []\n",
    "    for i in tqdm.tqdm(range(0,len(series),50)):\n",
    "        headers = {'Content-type': 'application/json'}\n",
    "        data = json.dumps({\"seriesid\": series[i:i+50],\n",
    "                           \"startyear\":\"2004\", \n",
    "                           \"endyear\":\"2016\", \n",
    "                           \"registrationKey\":regKeys[2]})\n",
    "        response = requests.post(bls_url, data=data, headers=headers)\n",
    "        json_data.append(json.loads(response.text))\n",
    "    return json_data\n",
    "\n",
    "def valid_config(config):\n",
    "    if 'prefix' in config:\n",
    "        if ('seasonal_adjustment' in config and config['seasonal_adjustment'] \n",
    "                                                                in ['S', 'U']):\n",
    "            if config['prefix'] == 'SM':\n",
    "                return ('state_code' in config \n",
    "                    and len(config['state_code']) == 2\n",
    "                    and 'area_code' in config \n",
    "                    and len(config['area_code']) == 5\n",
    "                    and 'supersector' in config \n",
    "                    and len(config['supersector']) == 2\n",
    "                    and 'industry' in config \n",
    "                    and len(config['industry']) == 6\n",
    "                    and 'datatype' in config \n",
    "                    and len(config['datatype']) == 2)\n",
    "\n",
    "            elif config['prefix'] == 'LA':\n",
    "                return ('state_area_code' in config \n",
    "                    and len(config['state_area_code']) == 15\n",
    "                    and 'measure' in config \n",
    "                    and config['measure'] in ['03','04','05','06'])\n",
    "    return False\n",
    "\n",
    "def format(config):\n",
    "    \"\"\"\n",
    "    brief:\n",
    "        - formats a request for data from the bls state and area employment, \n",
    "          hours, and earnings database if config['prefix'] == 'SM'\n",
    "\n",
    "        - formats a request for data from the bls local area unemployment \n",
    "          statistics database if config['prefix'] == 'SA'\n",
    "    args:\n",
    "        - config : {                   # http://www.bls.gov/help/hlpforma.htm#SM\n",
    "                    'prefix' : 'SM',\n",
    "                    'seasonal_adjustment' : 'S' or 'U',\n",
    "                    'state_code' : '01' to '50', \n",
    "                    'area_code' : '00000' to '99999',\n",
    "                    'supersector' : '00' to '99', \n",
    "                    'industry' : '000000' to '999999',     \n",
    "                    'datatype' : '00' to '99'              \n",
    "                    }\n",
    "                or {                   # http://www.bls.gov/help/hlpforma.htm#LA\n",
    "                    'prefix' : 'LA', \n",
    "                    'seasonal_adjustment' : 'S' or 'U',\n",
    "                    'area_type' : 'ST' or 'MT', \n",
    "                    'state_code' : '01' to '50',\n",
    "                    'area_code' : '000000000' or 'YYYYYYYYYYY'\n",
    "                    'measure': '03' to '06'\n",
    "                   }\n",
    "    \"\"\"\n",
    "    if valid_config(config):\n",
    "        # state and area employment, hours, and earnings\n",
    "        if config['prefix'] == 'SM':\n",
    "            # not using d.keys() here because keys must be in specific order\n",
    "            keys = ['prefix','seasonal_adjustment','state_code',\n",
    "                    'area_code','supersector','industry','datatype']\n",
    "            series_id = join([config[key] for key in keys], sep=\"\")\n",
    "        # local area unemployment statistics\n",
    "        elif config['prefix'] == 'LA':\n",
    "            # not using d.keys() here because keys must be in specific order\n",
    "            keys = ['prefix', 'seasonal_adjustment',\n",
    "                    'state_area_code', 'measure']\n",
    "            series_id = join([config[key] for key in keys], sep=\"\")\n",
    "        else:\n",
    "            raise Exception(\"KeyError: unsupported prefix\")\n",
    "        return series_id\n",
    "    else:\n",
    "        raise Exception(\"KeyError: invalid parameters\")\n",
    "\n",
    "\n",
    "def valid_configs(config):\n",
    "    if 'prefix' in config and config['prefix'] in [['SM'], ['LA']]:\n",
    "        if ('seasonal_adjustments' in config and config['seasonal_adjustments'] \n",
    "                                          in [['S'],['U'],['S','U'],['U','S']]):\n",
    "            if config['prefix'] == ['SM']:\n",
    "                return ('state_codes' in config \n",
    "                    and (reduce(opand, [0 < int(state_code) \n",
    "                        and int(state_code) <= 50 and len(state_code) == 2\n",
    "                            for state_code in config['state_codes']]))\n",
    "                    and ('area_codes' in config \n",
    "                        and reduce(opand, [len(area_code) == 5 \n",
    "                            for area_code in config['area_codes']]))\n",
    "                    and ('supersectors' in config\n",
    "                        and reduce(opand, [len(supersector) == 2 \n",
    "                            for supersector in config['supersectors']]))\n",
    "                    and ('industries' in config \n",
    "                        and reduce(opand, [len(industry) == 6 \n",
    "                            for industry in config['industries']]))\n",
    "                    and 'datatypes' in config)\n",
    "            elif config['prefix'] == ['LA']:\n",
    "                return (('state_area_codes' in config \n",
    "                    and (reduce(opand, [len(state_area_code) == 15\n",
    "                        for state_area_code in config['state_area_codes']])))\n",
    "                    and 'measures' in config) \n",
    "    return False\n",
    "\n",
    "def make_series(config):\n",
    "    \"\"\"\n",
    "    brief:\n",
    "        - provided a dictionary of bls series id parameters,\n",
    "          construct and request each possible series_id given parameters\n",
    "    args:\n",
    "        - config : {\n",
    "                    'prefix' : ['SM'],\n",
    "                    'seasonal_adjustments' : ['S' or 'U'],\n",
    "                    'state_codes' : [string],\n",
    "                    'area_codes' : [{string -> string}],\n",
    "                    'supersectors' : [string],\n",
    "                    'industries' : [string],\n",
    "                    'datatypes' : [string]\n",
    "                    }\n",
    "                or {\n",
    "                    'prefix' : ['LA'],\n",
    "                    'seasonal_adjustment' : ['S' or 'U'],\n",
    "                    'area_types' : ['ST' or 'MT'],\n",
    "                    'state_codes' : ['01' to '50']\n",
    "                    'area_codes' : [{'ST', 'xx' --> 'STxx00000000000', \n",
    "                                    'MT', 'xx' --> 'MTxxYYYYYYYYYYY'}]\n",
    "                    'measures': [string]\n",
    "                   }\n",
    "    \"\"\"\n",
    "    if valid_configs(config):\n",
    "        if config['prefix'] == ['SM']:\n",
    "            keys = ['prefix','seasonal_adjustment','state_code',\n",
    "                    'area_code','supersector','industry','datatype']\n",
    "            param_keys = ['prefix','seasonal_adjustments','state_codes',\n",
    "                          'area_codes','supersectors','industries','datatypes']\n",
    "        elif config['prefix'] == ['LA']:\n",
    "            keys = ['prefix','seasonal_adjustment','state_area_code','measure']\n",
    "            param_keys = ['prefix','seasonal_adjustments',\n",
    "                          'state_area_codes','measures']\n",
    "        else:\n",
    "            raise Exception(\"KeyError: unsupported prefix\")\n",
    "        return request([format(sid) for sid in nest(keys, param_keys, config)])\n",
    "    else:\n",
    "        raise Exception(\"KeyError: invalid parameters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, now we can actually use the API to get some useful information. Here, with a provided listed of metro codes (BLS codes corresponding to different metropolitan areas in the U.S.), and using those, we can construct queries to fetch various employment statistics from different states and metropolitan areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_codes = map(lambda i : str(i).zfill(2), range(1,51))\n",
    "\n",
    "with open('data/metro_codes.txt', 'r') as file:\n",
    "    splitline = lambda line : split(line, sep='\\t')[1]\n",
    "    metro_codes = map(splitline, split(file.read(), sep='\\n'))\n",
    "\n",
    "state_area_codes = {'ST' : dict(zip(state_codes, ['ST' + state_code + \n",
    "                                '00000000000' for state_code in state_codes])),\n",
    "                    'MT' : dict(zip(state_codes, metro_codes))\n",
    "                    }\n",
    "\n",
    "stateIndustry_config = {'prefix' : ['SM'],\n",
    "                        'seasonal_adjustments' : ['S', 'U'],\n",
    "                        'state_codes' : state_codes,\n",
    "                        'area_codes' : ['00000'],\n",
    "                        'supersectors' : ['20', '30', '40', '00'],\n",
    "                        'industries' : ['000000'],\n",
    "                        'datatypes' : ['01'],\n",
    "                        }\n",
    "\n",
    "state_config = {'prefix' : ['LA'],\n",
    "                'seasonal_adjustments' : ['S', 'U'],\n",
    "                'state_area_codes' : [state_area_codes['ST'][state_code] \n",
    "                                      for state_code in state_codes],\n",
    "                'measures' : ['05'], # employment\n",
    "                }\n",
    "\n",
    "stateMetro_config = {'prefix' : ['LA'],\n",
    "                     'seasonal_adjustments' : ['S', 'U'],\n",
    "                     'state_area_codes' : [state_area_codes['MT'][state_code] \n",
    "                                           for state_code in state_codes],\n",
    "                     'measures' : ['05'], # employment\n",
    "                     }\n",
    "\n",
    "stateIndustry_json_list = make_series(stateIndustry_config)\n",
    "\n",
    "print stateIndustry_json_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert = dict({'season_adjust' : {'S' : 'SA', 'U' : 'U'},\n",
    "                'state' : dict(zip(map(lambda i : \n",
    "                          str(i+1).zfill(2), range(50)), states)),\n",
    "                'industry' : {'20' : 'construction', \n",
    "                              '30' : 'manufactoring', \n",
    "                              '40' : 'transport_and_trade', \n",
    "                              '00' : 'all',\n",
    "                              },\n",
    "                'datatype' : {'01' : 'employees',\n",
    "                              },\n",
    "                'measure' : {'03' : 'unemployment_rate', \n",
    "                             '04' : 'unemployment', \n",
    "                             '05' : 'employment',\n",
    "                             '06' : 'labor_force',\n",
    "                             },\n",
    "                })\n",
    "keys = dict({'SM' : ['prefix','season_adjust','state',\n",
    "                     'industry','datatype'],\n",
    "             'LA' : ['prefix','season_adjust','state','datatype'],\n",
    "             })\n",
    "\n",
    "\n",
    "class Series(object):\n",
    "    def __init__(self, json_series):\n",
    "        self.attr = defaultdict(lambda : str)\n",
    "        self.attr.update({'prefix' : lambda sid : sid[0:2],\n",
    "                 'season_adjust' : lambda sid : convert['season_adjust'].get(sid[2]),\n",
    "                 'state' : lambda sid : convert['state'].get(sid[3:5] if sid[0:2] == \"SM\" else sid[5:7]),\n",
    "                 'industry' : lambda sid : convert['industry'].get(sid[10:12]),\n",
    "                 'datatype' : lambda sid : convert['datatype'].get(sid[18:20]) if sid[0:2] == \"SM\" else self.convert['measure'].get(sid[18:20]),\n",
    "                 })\n",
    "        series_id = json_series['seriesID']\n",
    "        self.set_attributes(series_id)\n",
    "        self.load_data(json_series['data'])\n",
    "\n",
    "    def set_attributes(self, series_id):\n",
    "        [setattr(self, key, self.attr[key](series_id)) for key in keys[series_id[0:2]]]\n",
    "\n",
    "    def load_data(self, json_data):\n",
    "        # industry -> year -> month -> state -> datatype -> season_adjust -> value\n",
    "        self.data = defaultdict(lambda : defaultdict(lambda : defaultdict(lambda : defaultdict(lambda : defaultdict(lambda : float)))))\n",
    "        for datum in json_data:\n",
    "            month = datum.get('periodName')\n",
    "            value = datum.get('value')\n",
    "            year = datum.get('year')\n",
    "            self.data[self.state][year][self.datatype][month][self.season_adjust] = float(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_dfs(json_data_list):\n",
    "    convert_industry = {'00' : 'all', '20' : 'construction', \n",
    "               '30' : 'manufactoring', '40' : 'transport_and_trade'}\n",
    "    industries = defaultdict(list)\n",
    "    industry_dfs = dict()\n",
    "    for json_data in json_data_list:\n",
    "        if json_data['Results'] != []:\n",
    "            for json_series in json_data['Results']['series']:\n",
    "                series_id = json_series['seriesID']\n",
    "                if series_id[0:2] != 'SM':\n",
    "                    raise Exception(\"unsupported query prefix\")\n",
    "                industry = convert_industry[series_id[10:12]]\n",
    "                industries[industry].append(Series(json_series))\n",
    "    for industry in industries.iterkeys():\n",
    "        df = pd.DataFrame()\n",
    "        data = defaultdict(list)\n",
    "        data_valid = defaultdict(set)\n",
    "        index = []\n",
    "        for series in industries[industry]:\n",
    "            d1 = deepcopy(series.data)\n",
    "            d2 = deepcopy(series.data)\n",
    "            d3 = deepcopy(series.data)\n",
    "            d4 = deepcopy(series.data)\n",
    "            d5 = deepcopy(series.data)\n",
    "            for state in series.data.iterkeys():\n",
    "                for year in d1[state].iterkeys():\n",
    "                    index.append(state)\n",
    "                    for datatype in d2[state][year].iterkeys():\n",
    "                        m_vals = []\n",
    "                        for month in d3[state][year][datatype].iterkeys():\n",
    "                            vals = []\n",
    "                            for adj in d4[state][year][datatype][month].iterkeys():\n",
    "                                val = d5[state][year][datatype][month][adj]\n",
    "                                vals.append(val)\n",
    "                            m_vals.append(0 if vals == [] else sum(vals) / len(vals))\n",
    "                        m_val = 0 if m_vals == [] else sum(m_vals) / len(m_vals)\n",
    "                        # (year, state, datatype) : avg val\n",
    "                        data_valid[year].add(m_val)\n",
    "                        data[year].append(m_val)\n",
    "                        \n",
    "        print len(data['2016'])\n",
    "        print data['2016']\n",
    "        print\n",
    "        assert(len(data['2016']) == len(data_valid['2016']))\n",
    "        industry_dfs[industry] = pd.DataFrame(data=data, index=index)\n",
    "    return industry_dfs\n",
    "\n",
    "dfs = make_dfs(stateIndustry_json_list)\n",
    "for key in dfs.iterkeys():\n",
    "    print \"%d industry dataframe: \" % (key)\n",
    "    print dfs[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the dataframes that will be input into the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_SVM_input(year, blue_collar_df, total_employment_df, df_4_years_ago):\n",
    "    df = pd.DataFrame({\"state\": states, \n",
    "                       \"blue_collar_change_last_8_years\": (blue_collar_df[year] - blue_collar_df[year - 8]) / blue_collar_df[year - 8],\n",
    "                       \"total_employment_change_last_8_years\": (total_employment_df[year] - total_employment_df[year - 8]) / total_employment_df[year - 8]})\n",
    "    df[\"proportion_blue_4_years_ago\"] = find_percent_blue(df[\"state\"], year)\n",
    "    return df\n",
    "\n",
    "df_predict_2012 = create_SVM_input(2012, blue_collar_df, total_employment_df, df_2008)\n",
    "df_predict_2016 = create_SVM_input(2016, blue_collar_df, total_employment_df, df_2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we label each state with its result (1 for Democrat, 0 for Republican) in each of the last 3 presidential elections (2008, 2012, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Alabama': [0,0,0], 'Alaska': [0,0,0],\n",
    "    'Arizona': [0,0,0], 'Arkansas': [0,0,0],\n",
    "    'California': [1,1,1], 'Colorado': [1,1,1],\n",
    "    'Connecticut': [1,1,1], 'Delaware': [1,1,1],\n",
    "    'District Of Columbia': [1,1,1], 'Florida': [1,1,0],\n",
    "    'Georgia': [0,0,0], 'Hawaii': [1,1,1],\n",
    "    'Idaho': [0,0,0], 'Illinois': [1,1,1],\n",
    "    'Indiana': [1,0,0], 'Iowa': [1,1,0],\n",
    "    'Kansas': [0,0,0], 'Kentucky': [0,0,0],\n",
    "    'Louisiana': [0,0,0], 'Maine': [1,1,1],\n",
    "    'Maryland': [1,1,1],'Massachusetts': [1,1,1],\n",
    "    'Michigan': [1,1,0],'Minnesota': [1,1,1],\n",
    "    'Mississippi': [0,0,0], 'Missouri': [0,0,0],\n",
    "    'Montana': [0,0,0], 'Nebraska': [0,0,0],\n",
    "    'Nevada': [1,1,1], 'New Hampshire': [1,1,1],\n",
    "    'New Jersey': [1,1,1],'New Mexico': [1,1,1],\n",
    "    'New York': [1,1,1], 'North Carolina': [1,0,0],\n",
    "    'North Dakota': [0,0,0],'Ohio': [1,1,0],    \n",
    "    'Oklahoma': [0,0,0],'Oregon': [1,1,1],\n",
    "    'Pennsylvania': [1,1,0],'Rhode Island': [1,1,1],\n",
    "    'South Carolina': [0,0,0],'South Dakota': [0,0,0],\n",
    "    'Tennessee': [0,0,0],'Texas': [0,0,0],\n",
    "    'Utah': [0,0,0], 'Vermont': [1,1,1],\n",
    "    'Virginia': [1,1,1],'Washington': [1,1,1],\n",
    "    'West Virginia': [0,0,0], 'Wisconsin': [1,1,0],\n",
    "    'Wyoming': [0,0,0]\n",
    "}\n",
    "\n",
    "def provide_labels(year):\n",
    "    return np.array([v[(year - 2008) / 4] for k, v in results.iteritems()])\n",
    "    \n",
    "# df_predict_2012[\"alphabetical_state_results_2008\"] = provide_labels(2008)\n",
    "df_predict_2012[\"alphabetical_state_results_2012\"] = provide_labels(2012)\n",
    "df_predict_2016[\"alphabetical_state_results_2016\"] = provide_labels(2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted all the necessary training data, we train an SVM on the features and labels of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier for training examples, which are 2012 states and labels\n",
    "def learn_classifier(training_features, training_labels):\n",
    "    svc = sklearn.svm.SVC()\n",
    "    return svc.fit(training_features, training_labels)\n",
    "\n",
    "trained_svc_industry_to_total = learn_classifier(df_predict_2012[\"blue_collar_change_last_8_years\", \n",
    "                                               \"total_employment_change_last_8_years\"], \n",
    "                               df_predict_2012[\"alphabetical_state_results_2012\"])\n",
    "\n",
    "trained_svc_industry_to_vote = learn_classifier(df_predict_2012[\"blue_collar_change_last_8_years\",\n",
    "                                               \"proportion_blue_4_years_ago\"], \n",
    "                               df_predict_2012[\"alphabetical_state_results_2012\"])\n",
    "\n",
    "trained_svc_total_to_vote = learn_classifier(df_predict_2012[\"total_employment_change_last_8_years\", \n",
    "                                               \"proportion_blue_4_years_ago\"], \n",
    "                               df_predict_2012[\"alphabetical_state_results_2012\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run our classifier on our testing set, the states in the 2016 election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_classifier(svc, testing_features, testing_labels):\n",
    "    predicted_labels = svc.predict(testing_features)\n",
    "    return float(np.sum(predicted_labels == testing_labels)) / testing_features.shape[0]\n",
    "\n",
    "accuracy_industry_to_total = eval_classifier(trained_svc_industry_to_total, df_predict_2016[\"blue_collar_change_last_8_years\", \n",
    "                                                        \"total_employment_change_last_8_years\"], \n",
    "                           df[\"alphabetical_state_results_2016\"])\n",
    "\n",
    "accuracy_industry_to_vote = eval_classifier(trained_svc_industry_to_vote, df_predict_2016[\"blue_collar_change_last_8_years\",\n",
    "                                                        \"proportion_blue_4_years_ago\"], \n",
    "                           df[\"alphabetical_state_results_2016\"])\n",
    "\n",
    "accuracy_total_to_vote = eval_classifier(trained_svc_total_to_vote, df_predict_2016[\"total_employment_change_last_8_years\", \n",
    "                                                        \"proportion_blue_4_years_ago\"], \n",
    "                           df[\"alphabetical_state_results_2016\"])\n",
    "\n",
    "\n",
    "print accuracy_industry_to_total, accuracy_industry_to_vote, accuracy_total_to_vote"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
